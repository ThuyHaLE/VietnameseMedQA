{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install rank_bm25\n",
        "!pip -q install -U sentence-transformers\n",
        "!pip -q install deep_translator\n",
        "!pip -q install transformers==4.34.0 datasets==2.14.5 accelerate==0.23.0 evaluate==0.4.1"
      ],
      "metadata": {
        "id": "tuRRu6lp1QT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb86c35e-9510-4c36-a1f6-a5e1f384d41d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m81.9/86.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Inference\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "HK3PmEHf1QQt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set-up\n",
        "#Corpus - download corpus\n",
        "!gdown 1GrRGfYIaVm48opRgBF0L4RbS7sHwX2Jw #Vi-En ver.\n",
        "\n",
        "#Function - data preprocessing\n",
        "!gdown 1PgMXgc1ZJ-sqz-XR1bFu2cdR1q4kXu_P\n",
        "from context_retrieval_vimedqa import *"
      ],
      "metadata": {
        "id": "l2pQtIsLYF6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bm25_searcher(test, corpus, top_k=1):\n",
        "  words = corpus['context_token'].tolist()\n",
        "  bm25 = BM25Okapi(words)\n",
        "  indexs = list(range(len(words)))\n",
        "  contexts = []\n",
        "  for question_index in range(len(test)):\n",
        "    top_docs = []\n",
        "    tokenized_query = test.iloc[question_index]['ques_token']\n",
        "    ##### BM25 search (lexical search) #####\n",
        "    top_indexs = bm25.get_top_n(tokenized_query, indexs, n=top_k)\n",
        "    hits = [{'corpus_id' : index} for index in top_indexs]\n",
        "    query = test.iloc[question_index]['question']\n",
        "    ##### Re-Ranking #####\n",
        "    cross_inp = [[query, corpus['topic'].iloc[hit['corpus_id']] + '. ' + corpus['context'].iloc[hit['corpus_id']]] for hit in hits]\n",
        "    cross_scores = cross_encoder.predict(cross_inp)\n",
        "    for idx in range(len(cross_scores)):\n",
        "      hits[idx]['cross-score'] = cross_scores[idx]\n",
        "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
        "    for hit in hits[0:3]: #only top 3\n",
        "      top_docs.append(corpus['eng_context'].iloc[hit['corpus_id']].replace(\"\\n\", \" \"))\n",
        "    contexts.append('. '.join(top_docs))\n",
        "  test['eng_context'] = contexts\n",
        "  return test\n",
        "\n",
        "def translate_df(df):\n",
        "  df['question'] = df['question'].apply(translate_text)\n",
        "  df['option_1'] = df['option_1'].apply(translate_text)\n",
        "  df['option_2'] = df['option_2'].apply(translate_text)\n",
        "  df['option_3'] = df['option_3'].apply(translate_text)\n",
        "  df['option_4'] = df['option_4'].apply(translate_text)\n",
        "  df['option_5'] = df['option_5'].apply(translate_text)\n",
        "  df['option_6'] = df['option_6'].apply(translate_text)\n",
        "  df['eng_context'] = df['eng_context']\n",
        "  return df"
      ],
      "metadata": {
        "id": "xqOcaKx1CFz6"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model - download check point (trained on 2 epochs)\n",
        "!gdown 14oj4IyRdGKhoeysIihFUU8IrsvFD8EwC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4wXWmNB-Kq5",
        "outputId": "8a6aeb79-a7aa-4027-caf3-7c859a858565"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14oj4IyRdGKhoeysIihFUU8IrsvFD8EwC\n",
            "To: /content/flan-t5-base-finetuned-MedMCQA_FrenchMCQA-v1.pt\n",
            "100% 990M/990M [00:14<00:00, 68.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "8_e36l9uYk6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be126b80-9ffc-4c82-e06c-18b4b219b6e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "#Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "#Load model\n",
        "model.load_state_dict(torch.load(\"/content/flan-t5-base-finetuned-MedMCQA_FrenchMCQA-v1.pt\", map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_per_sample(df, model, tokenizer):\n",
        "  convert_to_binary = {'a': \"10000\", 'b':\"01000\", 'c':\"00100\", 'd':\"00010\", 'e':\"00001\",\n",
        "                       'ab': \"11000\", 'ac':\"10100\", 'ad':\"10010\", 'ae':\"10001\",\n",
        "                       'bc':\"01100\", 'bd':\"01010\", 'be':\"01001\", 'cd':\"00110\", 'ce':\"00101\", 'de':\"00011\",\n",
        "                       'abc': \"11100\", 'abd':\"11010\", 'abe':\"11001\", 'acd':\"10110\", 'ace':\"10101\", 'ade':\"10011\",\n",
        "                       'bcd': \"01110\", 'bce':\"01101\", 'bde':\"01011\", 'cde':\"00111\",\n",
        "                       'abcd':\"11110\", 'abce':\"11101\", 'abde':\"11011\", 'acde':\"10111\", 'bcde':\"01111\",'abcde':\"11111\"}\n",
        "  opt_no_lst = {2: 'AB', 3: 'ABC', 4: 'ABCD', 5: 'ABCDE', 6: 'ABCDEF', 7: 'ABCDEFG'}\n",
        "\n",
        "  answers = []\n",
        "  for idx, row in df.iterrows():\n",
        "    context = row[\"eng_context\"]\n",
        "    question = row[\"question\"]\n",
        "    options = [re.sub(r\"[A-E]\\.\", \"\", op).strip() for op in row[['option_1', 'option_2',\n",
        "                                                                 'option_3', 'option_4',\n",
        "                                                                 'option_5', 'option_6']].tolist() if str(op) != '']\n",
        "    options_format = ['%s. %s' % (i, option) for i, option in zip(opt_no_lst[len(options)], options)]\n",
        "\n",
        "    text_choices = \".\\n\".join(options_format)\n",
        "    prompt = f\"question: {question}. options: {text_choices}. context: {context}.\"\n",
        "\n",
        "    model_inputs = tokenizer(prompt, truncation=True, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**model_inputs, output_scores=True, max_new_tokens=30)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if prediction in convert_to_binary:\n",
        "      answer = convert_to_binary[prediction]\n",
        "    else:\n",
        "      answer = '00000'\n",
        "    answer = answer[:len(options)]\n",
        "    answers.append(answer)\n",
        "  return answers\n",
        "\n",
        "def prediction(test_data, corpus_df, model, tokenizer, top_k=5):\n",
        "  test_w_context_data = bm25_searcher(test_data, corpus_df, top_k=top_k)\n",
        "  #test_w_context_data.to_csv(\"vi_context_public_test.csv\", index = False)\n",
        "  en_test_w_context_data = translate_df(test_w_context_data)\n",
        "  #en_test_w_context_data.to_csv(\"en_context_public_test.csv\")\n",
        "  return predict_per_sample(en_test_w_context_data, model, tokenizer)"
      ],
      "metadata": {
        "id": "IzJTT4wJaYsH"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load corpus\n",
        "corpus_path = '/content/ViMed_corpus_dict_envi.csv'\n",
        "corpus_df = corpus_preprocessing(corpus_path)"
      ],
      "metadata": {
        "id": "O4wGS3sk-QT5"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing\n",
        "test_df_path = \"./public_test.csv\"\n",
        "test_data = test_set_preprocessing(test_df_path)"
      ],
      "metadata": {
        "id": "PXagsDT7MBvs"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction\n",
        "answers = prediction(test_data, corpus_df, model, tokenizer, top_k=5)"
      ],
      "metadata": {
        "id": "NC7ELZ_TL0FH"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['answer'] = answers\n",
        "result_df = test_data[[\"id\", \"answer\"]]\n",
        "result_df.to_csv(\"answer_public_test_colab_v1-envi-3.csv\")"
      ],
      "metadata": {
        "id": "hYFW40ZDYDpv"
      },
      "execution_count": 75,
      "outputs": []
    }
  ]
}