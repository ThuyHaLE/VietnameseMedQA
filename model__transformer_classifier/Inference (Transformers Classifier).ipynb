{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPuXN9TUgiBAAxVFMxV1Ukr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5FouuPo49yAb","executionInfo":{"status":"ok","timestamp":1699601203276,"user_tz":-420,"elapsed":19408,"user":{"displayName":"Hạ Lê (翠夏)","userId":"01819704699029854720"}},"outputId":"029039a0-2e04-45c3-be2b-96acfa7026d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q rank_bm25\n","!pip -q install -U sentence-transformers\n","!pip -q install deep_translator\n","!pip install -q transformers==4.34.0 datasets==2.14.5 accelerate==0.23.0 evaluate==0.4.1 peft==0.5.0"]},{"cell_type":"code","source":["#Inference\n","import torch\n","import numpy as np\n","import pandas as pd\n","from context_retrieval.context_retrieval_vimedqa import *\n","\n","from tqdm import tqdm\n","from collections import Counter\n","from datasets import load_dataset\n","\n","import transformers\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from transformers import TextClassificationPipeline"],"metadata":{"id":"lV0IuTEDHvrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def id_labeling(num_opts):\n","  option_dict = {2: ['a', 'b'],\n","               3: ['a', 'b', 'c','ab', 'ac', 'bc','abc'],\n","               4: ['a', 'b', 'c', 'd', 'ab', 'ac', 'ad', 'bc', 'bd', 'cd','abc', 'abd', 'acd', 'bcd','abcd'],\n","               5: ['a', 'b', 'c', 'd', 'e', 'ab', 'ac', 'ad', 'ae', 'bc', 'bd', 'be', 'cd', 'ce', 'de', 'abc', 'abd', 'abe', 'acd',\n","                   'ace', 'ade', 'bcd', 'bce', 'bde', 'cde', 'abcd', 'abce', 'abde', 'acde', 'bcde','abcde']}\n","  if num_opts in [2,3,4,5]:\n","    label_list = option_dict[num_opts]\n","    id2label = {}\n","    label2id = {}\n","    for idx, label in enumerate(label_list):\n","      id2label[idx] = label\n","      label2id[label] = idx\n","    num_labels = len(id2label)\n","    return id2label, label2id, num_labels\n","\n","def predict_per_sample(df, model, tokenizer):\n","  BERT_CLS = \"[CLS]\"\n","  BERT_BOS = \"\"\n","  BERT_SEP = \"[SEP]\"\n","  BERT_EOS = \"\"\n","  convert_to_binary = {'a': \"10000\", 'b':\"01000\", 'c':\"00100\", 'd':\"00010\", 'e':\"00001\",\n","                       'ab': \"11000\", 'ac':\"10100\", 'ad':\"10010\", 'ae':\"10001\",\n","                       'bc':\"01100\", 'bd':\"01010\", 'be':\"01001\", 'cd':\"00110\", 'ce':\"00101\", 'de':\"00011\",\n","                       'abc': \"11100\", 'abd':\"11010\", 'abe':\"11001\", 'acd':\"10110\", 'ace':\"10101\", 'ade':\"10011\",\n","                       'bcd': \"01110\", 'bce':\"01101\", 'bde':\"01011\", 'cde':\"00111\",\n","                       'abcd':\"11110\", 'abce':\"11101\", 'abde':\"11011\", 'acde':\"10111\", 'bcde':\"01111\",'abcde':\"11111\"}\n","  id2label, label2id, num_labels = id_labeling(5)\n","  answers = []\n","  for key, d in df.iterrows():\n","    if str(d[\"context\"]) == \"nan\":\n","      d[\"context\"] = \"None\"\n","\n","    opt_lst = [op for op in d[['option_1', 'option_2', 'option_3', 'option_4', 'option_5', 'option_6']].tolist() if str(op) != '']\n","    bert_ctx = BERT_CLS + \" \" + d[\"question\"] + f\" {BERT_SEP} \" + f\" {BERT_SEP} \".join(opt_lst) + \" \" + BERT_SEP + d[\"context\"] + BERT_EOS\n","\n","    # Prediction\n","    res = pipeline(bert_ctx, truncation=True, max_length=model.config.max_position_embeddings)\n","    lb = res[0][0][\"label\"].split(\"_\")[-1]\n","    pred = id2label[int(lb)]\n","    answer = str(convert_to_binary[pred])\n","    #answer = convert_to_binary[prediction.item()]\n","    answer = answer[:len(opt_lst)-1]\n","    answers.append('{}'.format(answer))\n","  return answers\n","\n","def prediction(test_df_path, model, tokenizer):\n","  test_data = test_set_preprocessing(test_df_path)\n","  corpus_path = './modified_dataset/ViMed_corpus_dict.csv'\n","  corpus_df = corpus_preprocessing(corpus_path)\n","  test_w_context_data = bm25_searcher(test_data, corpus_df, top_k=5)\n","  #test_w_context_data.to_csv(\"vi_context_public_test.csv\", index = False)\n","  en_test_w_context_data = translate_df(test_w_context_data)\n","  #en_test_w_context_data.to_csv(\"en_context_public_test.csv\")\n","  return predict_per_sample(en_test_w_context_data, model, tokenizer)"],"metadata":{"id":"JJZShmXIFbUD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path_model = \".model__transformer_classifier/scibert_scivocab_uncased-finetuned-MedMCQA&FrenchMCQA-v7/checkpoint-30000\"\n","tokenizer = AutoTokenizer.from_pretrained(path_model)\n","model = AutoModelForSequenceClassification.from_pretrained(path_model)\n","pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=1, device='cpu')\n","\n","test_df_path = \"./modified_dataset/public_test.csv\"\n","\n","answers = prediction(test_df_path, model, tokenizer)\n","answers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7-HJnyRP6a_","executionInfo":{"status":"ok","timestamp":1699604687126,"user_tz":-420,"elapsed":140695,"user":{"displayName":"Hạ Lê (翠夏)","userId":"01819704699029854720"}},"outputId":"f85a3fb2-7d4e-4c9b-ebee-0fb5c242814b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","6it [02:00, 20.14s/it]\n"]},{"output_type":"execute_result","data":{"text/plain":["['001', '001', '001', '0', '010', '011']"]},"metadata":{},"execution_count":47}]}]}